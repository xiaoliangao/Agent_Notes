### 第三章 大语言模型基础

#### 3.1 语言模型和Transformer架构

N-gram模型是第n个词出现的概率只与前n-1个词有关

Embedding模型：建立一个语义空间，学习从上下文到下一个词的映射；通过余弦相似度来衡量相关性

RNN：为网络增加”记忆能力“

长短时记忆网络：引入“**细胞状态**”和“**门控机制**”

- 遗忘门 ：决定从上一时刻的细胞状态中丢弃哪些信息。
- 输入门 ：决定将当前输入中的哪些新信息存入细胞状态。
- 输出门 ：决定根据当前的细胞状态，输出哪些信息到隐藏状态。

**Transformer 架构：**基于**自注意力机制**

- 编码器 (Encoder) ：任务是“理解”输入的整个句子。它会读取所有输入词元(这个概念会在3.2.2节介绍)，最终为每个词元生成一个富含上下文信息的向量表示。
- 解码器 (Decoder) ：任务是“生成”目标句子。它会参考自己已经生成的前文，并“咨询”编码器的理解结果，来生成下一个词。

1. 查询 (Query, Q)：代表当前词元，它正在主动地“查询”其他词元以获取信息。
2. 键 (Key, K)：代表句子中可被查询的词元“标签”或“索引”。
3. 值 (Value, V)：代表词元本身所携带的“内容”或“信息”。

Q、K、V是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵 (W^Q^, W^K^, W^V^) 得到，过程如下：

- 准备“考题”和“资料”：对于句子中的每个词，都通过权重矩阵生成其$Q,K,V$向量。
- 计算相关性得分：要计算词$A$的新表示，就用词$A$的$Q$向量，去和句子中所有词（包括$A$自己）的$K$向量进行点积运算。这个得分反映了其他词对于理解词$A$的重要性。
- 稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_{k}}$（$d_{k}$是$K$向量的维度），以防止梯度过小，然后用Softmax函数将分数转换成总和为1的权重，也就是归一化的过程。
- 加权求和：将上一步得到的权重分别乘以每个词对应的$V$向量，然后将所有结果相加。最终得到的向量，就是词$A$融合了全局上下文信息后的新表示。

$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V
$$

**多头注意力机制**就是将向量在维度上切分成h（头数）份

**Decoder-Only 架构：**是一种自回归的工作模式

1. 给模型一个起始文本（例如 “Datawhale Agent is”）。
2. 模型预测出下一个最有可能的词（例如 “a”）。
3. 模型将自己刚刚生成的词 “a” 添加到输入文本的末尾，形成新的输入（“Datawhale Agent is a”）。
4. 模型基于这个新输入，再次预测下一个词（例如 “powerful”）。
5. 不断重复这个过程，直到生成完整的句子或达到停止条件。

在生成过程中，使用Mask Self-Attention保证不去偷看下一个词的答案（将当前位置之后的词元分数置为非常大的负数）

#### 3.2 与LLM交互

**提示工程：**研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复*

`Temperature`：控制模型回答随机性—确定性的参数，值越低，确定性越高；值越高，创新性越好

`Top-k`：将所有 token 按概率从高到低排序，排名前 k 个的 token 组成 “候选集”

- 与温度采样的区别与联系：温度采样通过温度 T 调整所有 token 的概率分布（平滑或陡峭），不改变候选token 的数量（仍考虑全部 N 个）。Top-k 采样通过 k 值限制候选 token 的数量（只保留前 k 个高概率token），再从其中采样。当k=1时输出完全确定，退化为 “贪心采样”。

`Top-p`：其原理是将所有 token 按概率从高到低排序，从排序后的第一个 token 开始，逐步累加概率，直到累积和首次达到或超过阈值 p

- 与Top-k的区别与联系：相对于固定截断大小的 Top-k，Top-p 能动态适应不同分布的“长尾”特性，对概率分布不均匀的极端情况的适应性更好。

流程：温度调整整体分布的陡峭程度，Top-k 会先保留概率最高的 k 个候选，然后 Top-p 会从 Top-k 的结果中选取累积概率≥p 的最小集合作为最终的候选集

样本提示：零样本提示、单样本提示、少样本提示

**指令调优 (Instruction Tuning)** 是一种微调技术，它使用大量“指令-回答”格式的数据对预训练模型进行进一步的训练

**提示技巧：**

- 角色扮演 (Role-playing) 通过赋予模型一个特定的角色，我们可以引导它的回答风格、语气和知识范围，使其输出更符合特定场景的需求。
- 上下文示例 (In-context Example) 这与少样本提示的思想一致，通过在提示中提供清晰的输入输出示例，来“教会”模型如何处理我们的请求，尤其是在处理复杂格式或特定风格的任务时非常有效。

**思维链：**引导模型“一步一步地思考”，提升了模型在复杂任务上的推理能力。通过显式地展示其推理过程，模型不仅更容易得出正确的答案，也让它的回答变得更可信、更易于我们检查和纠正。

选择模型的参考标准：性能与能力、成本、速度、上下文窗口、部署方式、生态和工具链、可微调醒与定制化、安全性与伦理

#### 3.3 大语言模型的缩放法则与局限性

**缩放法则：**揭示了模型性能与模型参数量、训练数据量以及计算资源之间存在着可预测的幂律关系，我们持续、按比例地增加这三个要素，模型的性能就会可预测地、平滑地提升，而不会出现明显的瓶颈。

**模型幻觉：**模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件

- 事实性幻觉 ： 模型生成与现实世界事实不符的信息。
- 忠实性幻觉 ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。
- 内在幻觉 ： 模型生成的内容与输入信息直接矛盾。

检测和缓解方式：

1. 数据层面： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式[13]，从源头减少幻觉。
2. 模型层面： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。
3. 推理与生成层面：
   1. 检索增强生成： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。
   2. 多步推理与验证： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。
   3. 引入外部工具： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。

---

#### 习题：

1. 自然语言处理中，语言模型经历了从统计到神经网络的模型演进。

   - 请使用本章提供的迷你语料库（`datawhale agent learns`, `datawhale agent works`），计算句子 `agent works` 在Bigram模型下的概率
   - N-gram模型的核心假设是马尔可夫假设。请解释这个假设的含义，以及N-gram模型存在哪些根本性局限？
   - 神经网络语言模型（RNN/LSTM）和Transformer分别是如何克服N-gram模型局限的？它们各自的优势是什么？

   ```
   - 模型用有限窗口（固定长度）近似真实条件分布，忽略更远历史信息。
   - 上下文长度受限，token过长时，会遗忘前面的文本信息
   - 泛化能力差
   ```

2. Transformer架构[4]是现代大语言模型的基础。其中：

   > **提示**：可以结合本章3.1.2节的代码实现来辅助理解

   - 自注意力机制（Self-Attention）的核心思想是什么？
   - 为什么Transformer能够并行处理序列，而RNN必须串行处理？位置编码（Positional Encoding）在其中起什么作用？
   - Decoder-Only架构与完整的Encoder-Decoder架构有什么区别？为什么现在主流的大语言模型都采用Decoder-Only架构？

   ```
   - 对于某个词，会计划它与句子中其他所有词的关系（得分）
   - RNN需要hidden state的信息，但是Transformer不需要
   - 因为Decoder-Only就能完成生成任务，训练简单，通用性强
   ```

3. 文本子词分词算法是大语言模型的一项关键技术，负责将文本转换为模型可处理的 token 序列。那为什么不能直接以"字符"或"单词"作为模型的输入单元？BPE（Byte Pair Encoding）算法解决了什么问题？

   ```
   - 使用单词会造成词表变长，字符做输入会使得token序列变长
   - 从字符开始，反复合并语料中最频繁的连续符号对，直到达到预设的词表大小或停止条件。最终得到既能表示常见完整词，也能拆分未知词为合理子单元的词表。
   ```

4. 本章3.2.3节介绍了如何本地部署开源大语言模型。请完成以下实践和分析：

   > **提示**：这是一道动手实践题，建议实际操作

   - 按照本章的指导，在本地部署一个轻量级的开源模型（推荐[Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B)），并尝试调整采样参数并观察其对输出的影响
   - 选择一个具体任务（如文本分类、信息抽取、代码生成等），设计并对比以下不同的提示策略（如Zero-shot、Few-shot、Chain-of-Thought）对输出结果的效果差异
   - 从性能、成本、可控性、隐私等维度比较闭源模型和开源模型
   - 如果你要构建一个企业级的客服智能体，你会选择哪种类型的模型？需要考虑哪些因素？

5. 模型幻觉（Hallucination）[11]是大语言模型当前存在的关键局限性之一。本章介绍了缓解幻觉的方法（如检索增强生成、多步推理、外部工具调用）

   - 请选择其中一种，说明其工作原理和适用场景
   - 调研前沿的研究和论文，是否还有其他的缓解模型幻觉的方法，他们又有哪些改进和优势？

   ```
   - RAG
   - 原理：构建专门的知识库，用户提出问题也转换成知识向量，这样LLM根据语义向量去知识库找相似的语义向量，这样能保证答案的正确性
   - 适用于：需要事实支持的问答（法律、医疗）
   - 多步推理 + 验证、基于证据的生成
   ```

6. 假设你要设计一个论文辅助阅读智能体，它能够帮助研究人员快速阅读并理解学术论文，包括：总结论文研究的核心内容、回答关于论文的问题、提取关键信息、比较多篇不同论文的观点等。请回答：

   - 你会选择哪个模型作为智能体设计时的基座模型？选择时需要考虑哪些因素？
   - 如何设计提示词来引导模型更好地理解学术论文？学术论文通常很长，可能超过模型的上下文窗口限制，你会如何解决这个问题？
   - 学术研究是严谨的，这意味着我们需要确保智能体生成的信息是准确客观忠于原文的。你认为系统中加入哪些设计能够更好的实现这一需求？

   ```
   - GPT、GEMINI
   - 你是一个专业的论文阅读助手。下面是论文的若干段落（我会给出段落编号）。请完成：
     1) 提供≤3句话的论文核心贡献（直接引用段落编号作为证据）。
     2) 列出用于支持主要结论的关键方法或实验（如果有，提供对应表/图/段落编号）。
     3) 给出论文的主要假设与局限（用论文原文位置作为依据）。
     4) 回答用户指定问题（并返回证据段落与页码）。
     始终在每条断言后注明【段落编号/页码/行号】作为证据。如果无法从给定文本直接得出，应回复“文中未给出”。
   - 使用RAG
   - 给出强制性要求：始终在每条断言后注明【段落编号/页码/行号】作为证据。如果无法从给定文本直接得出，应回复“文中未给出”。
   ```

---

